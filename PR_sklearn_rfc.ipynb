{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OONmhPHxTCew"
      },
      "source": [
        "# Pattern Recognition 24H1\n",
        "#### Runze Ji, Jiashuo Tian, Ziqian Liu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import necessary Modules\n",
        "* pandas\n",
        "* scikit-learn\n",
        "* itertools.islice\n",
        "* tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "08wd7jgWOOmk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import islice\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Specify the parameters used in training the classifier:\n",
        "* Number of training files (TRAIN_FILES_COUNT)\n",
        "* Epochs (EPOCHS)\n",
        "* Path to train files (TRAIN_FILES_PATH)\n",
        "* Path to Model (MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[init] Found 18329 Training Files\n",
            "\n"
          ]
        }
      ],
      "source": [
        "TRAIN_FILES_COUNT = 10000\n",
        "TRAIN_FILES_OFFSET = 0\n",
        "MEMORY_CLEANING = True\n",
        "\n",
        "TRAIN_FILES_PATH = '../../PR/train'\n",
        "MODEL_PATH = '../../PR/rfcModel.ptm'\n",
        "LOGS_PATH = '../../PR/rfcEval.csv'\n",
        "\n",
        "TRAIN_FILES = os.listdir(TRAIN_FILES_PATH)\n",
        "TRAIN_FILES_END = TRAIN_FILES_COUNT + TRAIN_FILES_OFFSET\n",
        "\n",
        "print(f'[init] Found {len(TRAIN_FILES)} Training Files\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Slice training files and encode labels\n",
        "* All files containing datasets will be sliced in specified count, allowing separate training\n",
        "* Encode 'type'\n",
        "* Show Correspondence between label and encoded label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "19GMj6YsOylz",
        "outputId": "d8d0c30f-0bfa-4665-8acc-9fabfae412b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[init] Reading from Index-0 to Index-9999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[preproc.loadCSV] Loading CSV Files...: 10000it [01:06, 150.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: 刺网 --> Encoded Label: 0\n",
            "Label: 围网 --> Encoded Label: 1\n",
            "Label: 拖网 --> Encoded Label: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(f'[init] Reading from Index-{TRAIN_FILES_OFFSET} to Index-{TRAIN_FILES_END-1}')\n",
        "train_files_pb = tqdm(islice(TRAIN_FILES, TRAIN_FILES_OFFSET, TRAIN_FILES_END), '[preproc.loadCSV] Loading CSV Files...')\n",
        "\n",
        "all_labels = []\n",
        "\n",
        "for file in train_files_pb:\n",
        "    file_path = os.path.join(TRAIN_FILES_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "    all_labels.extend(data['type'].unique())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_labels)\n",
        "\n",
        "# 打印标签和对应的编码\n",
        "for label, encoded_label in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
        "    print(f\"Label: {label} --> Encoded Label: {encoded_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Transforms dataframe and extend datatypes\n",
        "* Analyzes dataframe and extract features\n",
        "* Extend features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwxeI603PDzr",
        "outputId": "33580d52-3dee-42dc-89b1-ea42fe05649a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[preproc.transform] Transforming Data...:   2%|▏         | 201/10000 [00:03<03:09, 51.62it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_9084\\1805563178.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  20%|█▉        | 1957/10000 [00:27<02:24, 55.51it/s] C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_9084\\1805563178.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  28%|██▊       | 2754/10000 [00:40<02:06, 57.16it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_9084\\1805563178.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  32%|███▏      | 3243/10000 [00:47<01:47, 63.13it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_9084\\1805563178.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  47%|████▋     | 4695/10000 [01:08<00:59, 88.85it/s] C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_9084\\1805563178.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  69%|██████▉   | 6925/10000 [01:42<00:48, 63.16it/s] C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_9084\\1805563178.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  72%|███████▏  | 7156/10000 [01:45<00:39, 72.23it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_9084\\1805563178.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  79%|███████▉  | 7901/10000 [01:56<00:23, 90.11it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_9084\\1805563178.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  79%|███████▉  | 7911/10000 [01:56<00:22, 91.78it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_9084\\1805563178.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...: 100%|██████████| 10000/10000 [02:24<00:00, 69.11it/s]\n"
          ]
        }
      ],
      "source": [
        "X_all = []\n",
        "y_all = []\n",
        "\n",
        "train_files_pb = tqdm(islice(TRAIN_FILES, TRAIN_FILES_COUNT),'[preproc.transform] Transforming Data...', TRAIN_FILES_COUNT)\n",
        "\n",
        "for file in train_files_pb:\n",
        "    file_path = os.path.join(TRAIN_FILES_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # 转换时间列，提取特征等\n",
        "    data['time'] = pd.to_datetime(data['time'])\n",
        "    data['hour'] = data['time'].dt.hour\n",
        "    data['day_of_week'] = data['time'].dt.dayofweek\n",
        "    data['month'] = data['time'].dt.month\n",
        "\n",
        "    # 使用转换后的标签\n",
        "    data['type_encoded'] = label_encoder.transform(data['type'])\n",
        "\n",
        "    X = data[['lat', 'lon', '速度', '方向', 'hour', 'day_of_week', 'month']]\n",
        "    y = data['type_encoded']\n",
        "\n",
        "    X_all.append(X)\n",
        "    y_all.append(y)\n",
        "\n",
        "# 将所有数据合并为一个大的 DataFrame\n",
        "X = pd.concat(X_all, ignore_index=True)\n",
        "y = pd.concat(y_all, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Splitting the Dataset into train set and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KmR6JQPHPLyF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "if MEMORY_CLEANING:\n",
        "    X = 0\n",
        "    X_all = 0\n",
        "    y = 0\n",
        "    y_all = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initialize Random Forest Classifier\n",
        "* Train Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XLgbYJ7tPQet"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "building tree 1 of 80\n",
            "building tree 2 of 80\n",
            "building tree 3 of 80\n",
            "building tree 4 of 80\n",
            "building tree 5 of 80\n",
            "building tree 6 of 80\n",
            "building tree 7 of 80\n",
            "building tree 8 of 80\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m rf_classifier \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrf_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jirun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jirun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
            "File \u001b[1;32mc:\\Users\\jirun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jirun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jirun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\jirun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=80, random_state=42, verbose=2, n_jobs=8)\n",
        "rf_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test prediction accuracy on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ce58116aPUvU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   45.5s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9472605479610833\n"
          ]
        }
      ],
      "source": [
        "y_pred = rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['RandomForestClassifier.joblib']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from joblib import dump, load\n",
        "dump(rf_classifier, '../../PR/RandomForestClassifier.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[verify] Found 4034 Test Files\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[preproc.transform] Transforming Data...: 100%|██████████| 4034/4034 [00:46<00:00, 87.57it/s] \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>渔船ID</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>速度</th>\n",
              "      <th>方向</th>\n",
              "      <th>hour</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18330</td>\n",
              "      <td>30.685278</td>\n",
              "      <td>122.232972</td>\n",
              "      <td>8.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18330</td>\n",
              "      <td>30.694222</td>\n",
              "      <td>122.241833</td>\n",
              "      <td>8.6</td>\n",
              "      <td>40.0</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18330</td>\n",
              "      <td>30.702194</td>\n",
              "      <td>122.252056</td>\n",
              "      <td>8.6</td>\n",
              "      <td>44.0</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18330</td>\n",
              "      <td>30.719944</td>\n",
              "      <td>122.269750</td>\n",
              "      <td>8.9</td>\n",
              "      <td>40.0</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18330</td>\n",
              "      <td>30.728972</td>\n",
              "      <td>122.278306</td>\n",
              "      <td>9.4</td>\n",
              "      <td>42.0</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6422980</th>\n",
              "      <td>22363</td>\n",
              "      <td>30.940000</td>\n",
              "      <td>123.557889</td>\n",
              "      <td>3.4</td>\n",
              "      <td>152.0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6422981</th>\n",
              "      <td>22363</td>\n",
              "      <td>30.935917</td>\n",
              "      <td>123.560111</td>\n",
              "      <td>3.2</td>\n",
              "      <td>156.0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6422982</th>\n",
              "      <td>22363</td>\n",
              "      <td>30.921333</td>\n",
              "      <td>123.573750</td>\n",
              "      <td>3.3</td>\n",
              "      <td>135.0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6422983</th>\n",
              "      <td>22363</td>\n",
              "      <td>30.918056</td>\n",
              "      <td>123.577806</td>\n",
              "      <td>3.6</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6422984</th>\n",
              "      <td>22363</td>\n",
              "      <td>30.915139</td>\n",
              "      <td>123.582250</td>\n",
              "      <td>3.4</td>\n",
              "      <td>120.0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6422985 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          渔船ID        lat         lon   速度     方向  hour  day_of_week  month\n",
              "0        18330  30.685278  122.232972  8.0   22.0     7            5     10\n",
              "1        18330  30.694222  122.241833  8.6   40.0     7            5     10\n",
              "2        18330  30.702194  122.252056  8.6   44.0     7            5     10\n",
              "3        18330  30.719944  122.269750  8.9   40.0     7            5     10\n",
              "4        18330  30.728972  122.278306  9.4   42.0     7            5     10\n",
              "...        ...        ...         ...  ...    ...   ...          ...    ...\n",
              "6422980  22363  30.940000  123.557889  3.4  152.0     3            6     10\n",
              "6422981  22363  30.935917  123.560111  3.2  156.0     3            6     10\n",
              "6422982  22363  30.921333  123.573750  3.3  135.0     3            6     10\n",
              "6422983  22363  30.918056  123.577806  3.6  130.0     3            6     10\n",
              "6422984  22363  30.915139  123.582250  3.4  120.0     3            6     10\n",
              "\n",
              "[6422985 rows x 8 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TEST_DATASET_PATH = '../../PR/test_dataset'\n",
        "TEST_FILES = os.listdir(TEST_DATASET_PATH)\n",
        "print(f'[verify] Found {len(TEST_FILES)} Test Files\\n')\n",
        "\n",
        "'''\n",
        "all_labels_test = []\n",
        "\n",
        "for file in test_files_pb:\n",
        "    file_path = os.path.join(TEST_DATASET_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "    all_labels_test.extend(data['type'].unique())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_labels_test)\n",
        "'''\n",
        "\n",
        "X_verify_all = []\n",
        "\n",
        "verify_files_pb = tqdm(TEST_FILES,'[preproc.transform] Transforming Data...')\n",
        "\n",
        "for file in verify_files_pb:\n",
        "    file_path = os.path.join(TEST_DATASET_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # 转换时间列，提取特征等\n",
        "    data['time'] = pd.to_datetime(data['time'])\n",
        "    data['hour'] = data['time'].dt.hour\n",
        "    data['day_of_week'] = data['time'].dt.dayofweek\n",
        "    data['month'] = data['time'].dt.month\n",
        "\n",
        "    X_verify = data[['渔船ID', 'lat', 'lon', '速度', '方向', 'hour', 'day_of_week', 'month']]\n",
        "    #X_verify = data[['lat', 'lon', '速度', '方向', 'hour', 'day_of_week', 'month']]\n",
        "\n",
        "    X_verify_all.append(X_verify)\n",
        "\n",
        "# 将所有数据合并为一个大的 DataFrame\n",
        "X_verify = pd.concat(X_verify_all, ignore_index=True)\n",
        "X_verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from joblib import load\n",
        "rf_classifier = load('../../PR/RandomForestClassifier.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.1min\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([1, 1, 0, ..., 1, 1, 1])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test_pred = rf_classifier.predict(X_verify[['lat', 'lon', '速度', '方向', 'hour', 'day_of_week', 'month']])\n",
        "y_test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_verify['type'] = y_test_pred\n",
        "#X_test.loc[X_test['prediction' == 0], 'prediction'] = '刺网'\n",
        "X_duplicates = X_verify[['渔船ID', 'type']].replace({0:\"刺网\", 1:\"围网\", 2:\"拖网\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>渔船ID</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18330</td>\n",
              "      <td>围网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1880</th>\n",
              "      <td>18331</td>\n",
              "      <td>刺网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2865</th>\n",
              "      <td>18332</td>\n",
              "      <td>围网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4429</th>\n",
              "      <td>18333</td>\n",
              "      <td>围网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6599</th>\n",
              "      <td>18334</td>\n",
              "      <td>拖网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6411160</th>\n",
              "      <td>22359</td>\n",
              "      <td>刺网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6412497</th>\n",
              "      <td>22360</td>\n",
              "      <td>围网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6415171</th>\n",
              "      <td>22361</td>\n",
              "      <td>拖网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6415964</th>\n",
              "      <td>22362</td>\n",
              "      <td>围网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6419118</th>\n",
              "      <td>22363</td>\n",
              "      <td>刺网</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4034 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          渔船ID type\n",
              "0        18330   围网\n",
              "1880     18331   刺网\n",
              "2865     18332   围网\n",
              "4429     18333   围网\n",
              "6599     18334   拖网\n",
              "...        ...  ...\n",
              "6411160  22359   刺网\n",
              "6412497  22360   围网\n",
              "6415171  22361   拖网\n",
              "6415964  22362   围网\n",
              "6419118  22363   刺网\n",
              "\n",
              "[4034 rows x 2 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_final = X_duplicates.drop_duplicates(subset='渔船ID', keep='first')\n",
        "X_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_final.to_csv('../../PR/submissions/rfc/submission_rfc.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

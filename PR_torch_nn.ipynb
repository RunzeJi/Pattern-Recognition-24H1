{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OONmhPHxTCew"
      },
      "source": [
        "# Pattern Recognition 24H1\n",
        "#### Runze Ji, Jiashuo Tian, Ziqian Liu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import necessary Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "08wd7jgWOOmk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import islice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Specify the parameters used in training the classifier:\n",
        "* Number of training files (TRAIN_FILES_COUNT)\n",
        "* Epochs (EPOCHS)\n",
        "* Path to train files (TRAIN_FILES_PATH)\n",
        "* Path to Model (MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_FILES_COUNT = 1000\n",
        "TRAIN_FILES_OFFSET = 0\n",
        "EPOCHS = 20\n",
        "TRAIN_FILES_PATH = '../../PR/train'\n",
        "MODEL_PATH = '../../PR/model.ptm'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[init] Found 18329 Training Files\n",
            "[init] Reading From 0 to 999\n"
          ]
        }
      ],
      "source": [
        "TRAIN_FILES = os.listdir(TRAIN_FILES_PATH)\n",
        "TRAIN_FILES_END = TRAIN_FILES_COUNT + TRAIN_FILES_OFFSET - 1\n",
        "print(f'[init] Found {len(TRAIN_FILES)} Training Files\\n[init] Reading From {TRAIN_FILES_OFFSET} to {TRAIN_FILES_END}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.csv\n",
            "10.csv\n",
            "100.csv\n",
            "1000.csv\n",
            "10000.csv\n",
            "10001.csv\n",
            "10002.csv\n",
            "10003.csv\n",
            "10004.csv\n",
            "10005.csv\n",
            "10006.csv\n",
            "10007.csv\n",
            "10008.csv\n",
            "10009.csv\n",
            "1001.csv\n",
            "10010.csv\n",
            "10011.csv\n",
            "10012.csv\n",
            "10013.csv\n",
            "10014.csv\n",
            "10015.csv\n",
            "10016.csv\n",
            "10017.csv\n",
            "10018.csv\n",
            "10019.csv\n",
            "1002.csv\n",
            "10020.csv\n",
            "10021.csv\n",
            "10022.csv\n",
            "10023.csv\n",
            "10024.csv\n",
            "10025.csv\n",
            "10026.csv\n",
            "10027.csv\n",
            "10028.csv\n",
            "10029.csv\n",
            "1003.csv\n",
            "10030.csv\n",
            "10031.csv\n",
            "10032.csv\n",
            "10033.csv\n",
            "10034.csv\n",
            "10035.csv\n",
            "10036.csv\n",
            "10037.csv\n",
            "10038.csv\n",
            "10039.csv\n",
            "1004.csv\n",
            "10040.csv\n",
            "10041.csv\n",
            "10042.csv\n",
            "10043.csv\n",
            "10044.csv\n",
            "10045.csv\n",
            "10046.csv\n",
            "10047.csv\n",
            "10048.csv\n",
            "10049.csv\n",
            "1005.csv\n",
            "10050.csv\n",
            "10051.csv\n",
            "10052.csv\n",
            "10053.csv\n",
            "10054.csv\n",
            "10055.csv\n",
            "10056.csv\n",
            "10057.csv\n",
            "10058.csv\n",
            "10059.csv\n",
            "1006.csv\n",
            "10060.csv\n",
            "10061.csv\n",
            "10062.csv\n",
            "10063.csv\n",
            "10064.csv\n",
            "10065.csv\n",
            "10066.csv\n",
            "10067.csv\n",
            "10068.csv\n",
            "10069.csv\n",
            "1007.csv\n",
            "10070.csv\n",
            "10071.csv\n",
            "10072.csv\n",
            "10073.csv\n",
            "10074.csv\n",
            "10075.csv\n",
            "10076.csv\n",
            "10077.csv\n",
            "10078.csv\n",
            "10079.csv\n",
            "1008.csv\n",
            "10080.csv\n",
            "10081.csv\n",
            "10082.csv\n",
            "10083.csv\n",
            "10084.csv\n",
            "10085.csv\n",
            "10086.csv\n",
            "10087.csv\n",
            "10088.csv\n",
            "10089.csv\n",
            "1009.csv\n",
            "10090.csv\n",
            "10091.csv\n",
            "10092.csv\n",
            "10093.csv\n",
            "10094.csv\n",
            "10095.csv\n",
            "10096.csv\n",
            "10097.csv\n",
            "10098.csv\n",
            "10099.csv\n",
            "101.csv\n",
            "1010.csv\n",
            "10100.csv\n",
            "10101.csv\n",
            "10102.csv\n",
            "10103.csv\n",
            "10104.csv\n",
            "10105.csv\n",
            "10106.csv\n",
            "10107.csv\n",
            "10108.csv\n",
            "10109.csv\n",
            "1011.csv\n",
            "10110.csv\n",
            "10111.csv\n",
            "10112.csv\n",
            "10113.csv\n",
            "10114.csv\n",
            "10115.csv\n",
            "10116.csv\n",
            "10117.csv\n",
            "10118.csv\n",
            "10119.csv\n",
            "1012.csv\n",
            "10120.csv\n",
            "10121.csv\n",
            "10122.csv\n",
            "10123.csv\n",
            "10124.csv\n",
            "10125.csv\n",
            "10126.csv\n",
            "10127.csv\n",
            "10128.csv\n",
            "10129.csv\n",
            "1013.csv\n",
            "10130.csv\n",
            "10131.csv\n",
            "10132.csv\n",
            "10133.csv\n",
            "10134.csv\n",
            "10135.csv\n",
            "10136.csv\n",
            "10137.csv\n",
            "10138.csv\n",
            "10139.csv\n",
            "1014.csv\n",
            "10140.csv\n",
            "10141.csv\n",
            "10142.csv\n",
            "10143.csv\n",
            "10144.csv\n",
            "10145.csv\n",
            "10146.csv\n",
            "10147.csv\n",
            "10148.csv\n",
            "10149.csv\n",
            "1015.csv\n",
            "10150.csv\n",
            "10151.csv\n",
            "10152.csv\n",
            "10153.csv\n",
            "10154.csv\n",
            "10155.csv\n",
            "10156.csv\n",
            "10157.csv\n",
            "10158.csv\n",
            "10159.csv\n",
            "1016.csv\n",
            "10160.csv\n",
            "10161.csv\n",
            "10162.csv\n",
            "10163.csv\n",
            "10164.csv\n",
            "10165.csv\n",
            "10166.csv\n",
            "10167.csv\n",
            "10168.csv\n",
            "10169.csv\n",
            "1017.csv\n",
            "10170.csv\n",
            "10171.csv\n",
            "10172.csv\n",
            "10173.csv\n",
            "10174.csv\n",
            "10175.csv\n",
            "10176.csv\n",
            "10177.csv\n",
            "10178.csv\n",
            "10179.csv\n",
            "1018.csv\n",
            "10180.csv\n",
            "10181.csv\n",
            "10182.csv\n",
            "10183.csv\n",
            "10184.csv\n",
            "10185.csv\n",
            "10186.csv\n",
            "10187.csv\n",
            "10188.csv\n",
            "10189.csv\n",
            "1019.csv\n",
            "10190.csv\n",
            "10191.csv\n",
            "10192.csv\n",
            "10193.csv\n",
            "10194.csv\n",
            "10195.csv\n",
            "10196.csv\n",
            "10197.csv\n",
            "10198.csv\n",
            "10199.csv\n",
            "102.csv\n",
            "1020.csv\n",
            "10200.csv\n",
            "10201.csv\n",
            "10202.csv\n",
            "10203.csv\n",
            "10204.csv\n",
            "10205.csv\n",
            "10206.csv\n",
            "10207.csv\n",
            "10208.csv\n",
            "10209.csv\n",
            "1021.csv\n",
            "10210.csv\n",
            "10211.csv\n",
            "10212.csv\n",
            "10213.csv\n",
            "10214.csv\n",
            "10215.csv\n",
            "10216.csv\n",
            "10217.csv\n",
            "10218.csv\n",
            "10219.csv\n",
            "1022.csv\n",
            "10220.csv\n",
            "10221.csv\n",
            "10222.csv\n",
            "10223.csv\n",
            "10224.csv\n",
            "10225.csv\n",
            "10226.csv\n",
            "10227.csv\n",
            "10228.csv\n",
            "10229.csv\n",
            "1023.csv\n",
            "10230.csv\n",
            "10231.csv\n",
            "10232.csv\n",
            "10233.csv\n",
            "10234.csv\n",
            "10235.csv\n",
            "10236.csv\n",
            "10237.csv\n",
            "10238.csv\n",
            "10239.csv\n",
            "1024.csv\n",
            "10240.csv\n",
            "10241.csv\n",
            "10242.csv\n",
            "10243.csv\n",
            "10244.csv\n",
            "10245.csv\n",
            "10246.csv\n",
            "10247.csv\n",
            "10248.csv\n",
            "10249.csv\n",
            "1025.csv\n",
            "10250.csv\n",
            "10251.csv\n",
            "10252.csv\n",
            "10253.csv\n",
            "10254.csv\n",
            "10255.csv\n",
            "10256.csv\n",
            "10257.csv\n",
            "10258.csv\n",
            "10259.csv\n",
            "1026.csv\n",
            "10260.csv\n",
            "10261.csv\n",
            "10262.csv\n",
            "10263.csv\n",
            "10264.csv\n",
            "10265.csv\n",
            "10266.csv\n",
            "10267.csv\n",
            "10268.csv\n",
            "10269.csv\n",
            "1027.csv\n",
            "10270.csv\n",
            "10271.csv\n",
            "10272.csv\n",
            "10273.csv\n",
            "10274.csv\n",
            "10275.csv\n",
            "10276.csv\n",
            "10277.csv\n",
            "10278.csv\n",
            "10279.csv\n",
            "1028.csv\n",
            "10280.csv\n",
            "10281.csv\n",
            "10282.csv\n",
            "10283.csv\n",
            "10284.csv\n",
            "10285.csv\n",
            "10286.csv\n",
            "10287.csv\n",
            "10288.csv\n",
            "10289.csv\n",
            "1029.csv\n",
            "10290.csv\n",
            "10291.csv\n",
            "10292.csv\n",
            "10293.csv\n",
            "10294.csv\n",
            "10295.csv\n",
            "10296.csv\n",
            "10297.csv\n",
            "10298.csv\n",
            "10299.csv\n",
            "103.csv\n",
            "1030.csv\n",
            "10300.csv\n",
            "10301.csv\n",
            "10302.csv\n",
            "10303.csv\n",
            "10304.csv\n",
            "10305.csv\n",
            "10306.csv\n",
            "10307.csv\n",
            "10308.csv\n",
            "10309.csv\n",
            "1031.csv\n",
            "10310.csv\n",
            "10311.csv\n",
            "10312.csv\n",
            "10313.csv\n",
            "10314.csv\n",
            "10315.csv\n",
            "10316.csv\n",
            "10317.csv\n",
            "10318.csv\n",
            "10319.csv\n",
            "1032.csv\n",
            "10320.csv\n",
            "10321.csv\n",
            "10322.csv\n",
            "10323.csv\n",
            "10324.csv\n",
            "10325.csv\n",
            "10326.csv\n",
            "10327.csv\n",
            "10328.csv\n",
            "10329.csv\n",
            "1033.csv\n",
            "10330.csv\n",
            "10331.csv\n",
            "10332.csv\n",
            "10333.csv\n",
            "10334.csv\n",
            "10335.csv\n",
            "10336.csv\n",
            "10337.csv\n",
            "10338.csv\n",
            "10339.csv\n",
            "1034.csv\n",
            "10340.csv\n",
            "10341.csv\n",
            "10342.csv\n",
            "10343.csv\n",
            "10344.csv\n",
            "10345.csv\n",
            "10346.csv\n",
            "10347.csv\n",
            "10348.csv\n",
            "10349.csv\n",
            "1035.csv\n",
            "10350.csv\n",
            "10351.csv\n",
            "10352.csv\n",
            "10353.csv\n",
            "10354.csv\n",
            "10355.csv\n",
            "10356.csv\n",
            "10357.csv\n",
            "10358.csv\n",
            "10359.csv\n",
            "1036.csv\n",
            "10360.csv\n",
            "10361.csv\n",
            "10362.csv\n",
            "10363.csv\n",
            "10364.csv\n",
            "10365.csv\n",
            "10366.csv\n",
            "10367.csv\n",
            "10368.csv\n",
            "10369.csv\n",
            "1037.csv\n",
            "10370.csv\n",
            "10371.csv\n",
            "10372.csv\n",
            "10373.csv\n",
            "10374.csv\n",
            "10375.csv\n",
            "10376.csv\n",
            "10377.csv\n",
            "10378.csv\n",
            "10379.csv\n",
            "1038.csv\n",
            "10380.csv\n",
            "10381.csv\n",
            "10382.csv\n",
            "10383.csv\n",
            "10384.csv\n",
            "10385.csv\n",
            "10386.csv\n",
            "10387.csv\n",
            "10388.csv\n",
            "10389.csv\n",
            "1039.csv\n",
            "10390.csv\n",
            "10391.csv\n",
            "10392.csv\n",
            "10393.csv\n",
            "10394.csv\n",
            "10395.csv\n",
            "10396.csv\n",
            "10397.csv\n",
            "10398.csv\n",
            "10399.csv\n",
            "104.csv\n",
            "1040.csv\n",
            "10400.csv\n",
            "10401.csv\n",
            "10402.csv\n",
            "10403.csv\n",
            "10404.csv\n",
            "10405.csv\n",
            "10406.csv\n",
            "10407.csv\n",
            "10408.csv\n",
            "10409.csv\n",
            "1041.csv\n",
            "10410.csv\n",
            "10411.csv\n",
            "10412.csv\n",
            "10413.csv\n",
            "10414.csv\n",
            "10415.csv\n",
            "10416.csv\n",
            "10417.csv\n",
            "10418.csv\n",
            "10419.csv\n",
            "1042.csv\n",
            "10420.csv\n",
            "10421.csv\n",
            "10422.csv\n",
            "10423.csv\n",
            "10424.csv\n",
            "10425.csv\n",
            "10426.csv\n",
            "10427.csv\n",
            "10428.csv\n",
            "10429.csv\n",
            "1043.csv\n",
            "10430.csv\n",
            "10431.csv\n",
            "10432.csv\n",
            "10433.csv\n",
            "10434.csv\n",
            "10435.csv\n",
            "10436.csv\n",
            "10437.csv\n",
            "10438.csv\n",
            "10439.csv\n",
            "1044.csv\n",
            "10440.csv\n",
            "10441.csv\n",
            "10442.csv\n",
            "10443.csv\n",
            "10444.csv\n",
            "10445.csv\n",
            "10446.csv\n",
            "10447.csv\n",
            "10448.csv\n",
            "10449.csv\n",
            "1045.csv\n",
            "10450.csv\n",
            "10451.csv\n",
            "10452.csv\n",
            "10453.csv\n",
            "10454.csv\n",
            "10455.csv\n",
            "10456.csv\n",
            "10457.csv\n",
            "10458.csv\n",
            "10459.csv\n",
            "1046.csv\n",
            "10460.csv\n",
            "10461.csv\n",
            "10462.csv\n",
            "10463.csv\n",
            "10464.csv\n",
            "10465.csv\n",
            "10466.csv\n",
            "10467.csv\n",
            "10468.csv\n",
            "10469.csv\n",
            "1047.csv\n",
            "10470.csv\n",
            "10471.csv\n",
            "10472.csv\n",
            "10473.csv\n",
            "10474.csv\n",
            "10475.csv\n",
            "10476.csv\n",
            "10477.csv\n",
            "10478.csv\n",
            "10479.csv\n",
            "1048.csv\n",
            "10480.csv\n",
            "10481.csv\n",
            "10482.csv\n",
            "10483.csv\n",
            "10484.csv\n",
            "10485.csv\n",
            "10486.csv\n",
            "10487.csv\n",
            "10488.csv\n",
            "10489.csv\n",
            "1049.csv\n",
            "10490.csv\n",
            "10491.csv\n",
            "10492.csv\n",
            "10493.csv\n",
            "10494.csv\n",
            "10495.csv\n",
            "10496.csv\n",
            "10497.csv\n",
            "10498.csv\n",
            "10499.csv\n",
            "105.csv\n",
            "1050.csv\n",
            "10500.csv\n",
            "10501.csv\n",
            "10502.csv\n",
            "10503.csv\n",
            "10504.csv\n",
            "10505.csv\n",
            "10506.csv\n",
            "10507.csv\n",
            "10508.csv\n",
            "10509.csv\n",
            "1051.csv\n",
            "10510.csv\n",
            "10511.csv\n",
            "10512.csv\n",
            "10513.csv\n",
            "10514.csv\n",
            "10515.csv\n",
            "10516.csv\n",
            "10517.csv\n",
            "10518.csv\n",
            "10519.csv\n",
            "1052.csv\n",
            "10520.csv\n",
            "10521.csv\n",
            "10522.csv\n",
            "10523.csv\n",
            "10524.csv\n",
            "10525.csv\n",
            "10526.csv\n",
            "10527.csv\n",
            "10528.csv\n",
            "10529.csv\n",
            "1053.csv\n",
            "10530.csv\n",
            "10531.csv\n",
            "10532.csv\n",
            "10533.csv\n",
            "10534.csv\n",
            "10535.csv\n",
            "10536.csv\n",
            "10537.csv\n",
            "10538.csv\n",
            "10539.csv\n",
            "1054.csv\n",
            "10540.csv\n",
            "10541.csv\n",
            "10542.csv\n",
            "10543.csv\n",
            "10544.csv\n",
            "10545.csv\n",
            "10546.csv\n",
            "10547.csv\n",
            "10548.csv\n",
            "10549.csv\n",
            "1055.csv\n",
            "10550.csv\n",
            "10551.csv\n",
            "10552.csv\n",
            "10553.csv\n",
            "10554.csv\n",
            "10555.csv\n",
            "10556.csv\n",
            "10557.csv\n",
            "10558.csv\n",
            "10559.csv\n",
            "1056.csv\n",
            "10560.csv\n",
            "10561.csv\n",
            "10562.csv\n",
            "10563.csv\n",
            "10564.csv\n",
            "10565.csv\n",
            "10566.csv\n",
            "10567.csv\n",
            "10568.csv\n",
            "10569.csv\n",
            "1057.csv\n",
            "10570.csv\n",
            "10571.csv\n",
            "10572.csv\n",
            "10573.csv\n",
            "10574.csv\n",
            "10575.csv\n",
            "10576.csv\n",
            "10577.csv\n",
            "10578.csv\n",
            "10579.csv\n",
            "1058.csv\n",
            "10580.csv\n",
            "10581.csv\n",
            "10582.csv\n",
            "10583.csv\n",
            "10584.csv\n",
            "10585.csv\n",
            "10586.csv\n",
            "10587.csv\n",
            "10588.csv\n",
            "10589.csv\n",
            "1059.csv\n",
            "10590.csv\n",
            "10591.csv\n",
            "10592.csv\n",
            "10593.csv\n",
            "10594.csv\n",
            "10595.csv\n",
            "10596.csv\n",
            "10597.csv\n",
            "10598.csv\n",
            "10599.csv\n",
            "106.csv\n",
            "1060.csv\n",
            "10600.csv\n",
            "10601.csv\n",
            "10602.csv\n",
            "10603.csv\n",
            "10604.csv\n",
            "10605.csv\n",
            "10606.csv\n",
            "10607.csv\n",
            "10608.csv\n",
            "10609.csv\n",
            "1061.csv\n",
            "10610.csv\n",
            "10611.csv\n",
            "10612.csv\n",
            "10613.csv\n",
            "10614.csv\n",
            "10615.csv\n",
            "10616.csv\n",
            "10617.csv\n",
            "10618.csv\n",
            "10619.csv\n",
            "1062.csv\n",
            "10620.csv\n",
            "10621.csv\n",
            "10622.csv\n",
            "10623.csv\n",
            "10624.csv\n",
            "10625.csv\n",
            "10626.csv\n",
            "10627.csv\n",
            "10628.csv\n",
            "10629.csv\n",
            "1063.csv\n",
            "10630.csv\n",
            "10631.csv\n",
            "10632.csv\n",
            "10633.csv\n",
            "10634.csv\n",
            "10635.csv\n",
            "10636.csv\n",
            "10637.csv\n",
            "10638.csv\n",
            "10639.csv\n",
            "1064.csv\n",
            "10640.csv\n",
            "10641.csv\n",
            "10642.csv\n",
            "10643.csv\n",
            "10644.csv\n",
            "10645.csv\n",
            "10646.csv\n",
            "10647.csv\n",
            "10648.csv\n",
            "10649.csv\n",
            "1065.csv\n",
            "10650.csv\n",
            "10651.csv\n",
            "10652.csv\n",
            "10653.csv\n",
            "10654.csv\n",
            "10655.csv\n",
            "10656.csv\n",
            "10657.csv\n",
            "10658.csv\n",
            "10659.csv\n",
            "1066.csv\n",
            "10660.csv\n",
            "10661.csv\n",
            "10662.csv\n",
            "10663.csv\n",
            "10664.csv\n",
            "10665.csv\n",
            "10666.csv\n",
            "10667.csv\n",
            "10668.csv\n",
            "10669.csv\n",
            "1067.csv\n",
            "10670.csv\n",
            "10671.csv\n",
            "10672.csv\n",
            "10673.csv\n",
            "10674.csv\n",
            "10675.csv\n",
            "10676.csv\n",
            "10677.csv\n",
            "10678.csv\n",
            "10679.csv\n",
            "1068.csv\n",
            "10680.csv\n",
            "10681.csv\n",
            "10682.csv\n",
            "10683.csv\n",
            "10684.csv\n",
            "10685.csv\n",
            "10686.csv\n",
            "10687.csv\n",
            "10688.csv\n",
            "10689.csv\n",
            "1069.csv\n",
            "10690.csv\n",
            "10691.csv\n",
            "10692.csv\n",
            "10693.csv\n",
            "10694.csv\n",
            "10695.csv\n",
            "10696.csv\n",
            "10697.csv\n",
            "10698.csv\n",
            "10699.csv\n",
            "107.csv\n",
            "1070.csv\n",
            "10700.csv\n",
            "10701.csv\n",
            "10702.csv\n",
            "10703.csv\n",
            "10704.csv\n",
            "10705.csv\n",
            "10706.csv\n",
            "10707.csv\n",
            "10708.csv\n",
            "10709.csv\n",
            "1071.csv\n",
            "10710.csv\n",
            "10711.csv\n",
            "10712.csv\n",
            "10713.csv\n",
            "10714.csv\n",
            "10715.csv\n",
            "10716.csv\n",
            "10717.csv\n",
            "10718.csv\n",
            "10719.csv\n",
            "1072.csv\n",
            "10720.csv\n",
            "10721.csv\n",
            "10722.csv\n",
            "10723.csv\n",
            "10724.csv\n",
            "10725.csv\n",
            "10726.csv\n",
            "10727.csv\n",
            "10728.csv\n",
            "10729.csv\n",
            "1073.csv\n",
            "10730.csv\n",
            "10731.csv\n",
            "10732.csv\n",
            "10733.csv\n",
            "10734.csv\n",
            "10735.csv\n",
            "10736.csv\n",
            "10737.csv\n",
            "10738.csv\n",
            "10739.csv\n",
            "1074.csv\n",
            "10740.csv\n",
            "10741.csv\n",
            "10742.csv\n",
            "10743.csv\n",
            "10744.csv\n",
            "10745.csv\n",
            "10746.csv\n",
            "10747.csv\n",
            "10748.csv\n",
            "10749.csv\n",
            "1075.csv\n",
            "10750.csv\n",
            "10751.csv\n",
            "10752.csv\n",
            "10753.csv\n",
            "10754.csv\n",
            "10755.csv\n",
            "10756.csv\n",
            "10757.csv\n",
            "10758.csv\n",
            "10759.csv\n",
            "1076.csv\n",
            "10760.csv\n",
            "10761.csv\n",
            "10762.csv\n",
            "10763.csv\n",
            "10764.csv\n",
            "10765.csv\n",
            "10766.csv\n",
            "10767.csv\n",
            "10768.csv\n",
            "10769.csv\n",
            "1077.csv\n",
            "10770.csv\n",
            "10771.csv\n",
            "10772.csv\n",
            "10773.csv\n",
            "10774.csv\n",
            "10775.csv\n",
            "10776.csv\n",
            "10777.csv\n",
            "10778.csv\n",
            "10779.csv\n",
            "1078.csv\n",
            "10780.csv\n",
            "10781.csv\n",
            "10782.csv\n",
            "10783.csv\n",
            "10784.csv\n",
            "10785.csv\n",
            "10786.csv\n",
            "10787.csv\n",
            "10788.csv\n",
            "10789.csv\n",
            "1079.csv\n",
            "10790.csv\n",
            "10791.csv\n",
            "10792.csv\n",
            "10793.csv\n",
            "10794.csv\n",
            "10795.csv\n",
            "10796.csv\n",
            "10797.csv\n",
            "10798.csv\n",
            "10799.csv\n",
            "108.csv\n",
            "1080.csv\n",
            "10800.csv\n",
            "10801.csv\n",
            "10802.csv\n",
            "10803.csv\n",
            "10804.csv\n",
            "10805.csv\n",
            "10806.csv\n",
            "10807.csv\n",
            "10808.csv\n",
            "10809.csv\n",
            "1081.csv\n",
            "10810.csv\n",
            "10811.csv\n",
            "10812.csv\n",
            "10813.csv\n",
            "10814.csv\n",
            "10815.csv\n",
            "10816.csv\n",
            "10817.csv\n",
            "10818.csv\n",
            "10819.csv\n",
            "1082.csv\n",
            "10820.csv\n",
            "10821.csv\n",
            "10822.csv\n",
            "10823.csv\n",
            "10824.csv\n",
            "10825.csv\n",
            "10826.csv\n",
            "10827.csv\n",
            "10828.csv\n",
            "10829.csv\n",
            "1083.csv\n",
            "10830.csv\n",
            "10831.csv\n",
            "10832.csv\n",
            "10833.csv\n",
            "10834.csv\n",
            "10835.csv\n",
            "10836.csv\n",
            "10837.csv\n",
            "10838.csv\n",
            "10839.csv\n",
            "1084.csv\n",
            "10840.csv\n",
            "10841.csv\n",
            "10842.csv\n",
            "10843.csv\n",
            "10844.csv\n",
            "10845.csv\n",
            "10846.csv\n",
            "10847.csv\n",
            "10848.csv\n",
            "10849.csv\n",
            "1085.csv\n",
            "10850.csv\n",
            "10851.csv\n",
            "10852.csv\n",
            "10853.csv\n",
            "10854.csv\n",
            "10855.csv\n",
            "10856.csv\n",
            "10857.csv\n",
            "10858.csv\n",
            "10859.csv\n",
            "1086.csv\n",
            "10860.csv\n",
            "10861.csv\n",
            "10862.csv\n",
            "10863.csv\n",
            "10864.csv\n",
            "10865.csv\n",
            "10866.csv\n",
            "10867.csv\n",
            "10868.csv\n",
            "10869.csv\n",
            "1087.csv\n",
            "10870.csv\n",
            "10871.csv\n",
            "10872.csv\n",
            "10873.csv\n",
            "10874.csv\n",
            "10875.csv\n",
            "10876.csv\n",
            "10877.csv\n",
            "10878.csv\n",
            "10879.csv\n",
            "1088.csv\n",
            "10880.csv\n",
            "10881.csv\n",
            "10882.csv\n",
            "10883.csv\n",
            "10884.csv\n",
            "10885.csv\n",
            "10886.csv\n",
            "10887.csv\n",
            "10888.csv\n",
            "10889.csv\n",
            "1089.csv\n",
            "10890.csv\n",
            "10891.csv\n",
            "10892.csv\n",
            "10893.csv\n",
            "10894.csv\n",
            "10895.csv\n",
            "10896.csv\n",
            "10897.csv\n",
            "10898.csv\n"
          ]
        }
      ],
      "source": [
        "TRAIN_FILES[1000]\n",
        "for index in range(1000):\n",
        "    print(TRAIN_FILES[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "19GMj6YsOylz",
        "outputId": "d8d0c30f-0bfa-4665-8acc-9fabfae412b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[preproc.loadCSV] Loading CSV Files...: 19it [00:00, 167.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[preproc.loadCSV] Loading CSV Files...: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[preproc.loadCSV] Loading CSV Files...: 999it [00:06, 163.43it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-4 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-4 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-4 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-4 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-4 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LabelEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_files_pb = tqdm(islice(TRAIN_FILES, TRAIN_FILES_OFFSET, TRAIN_FILES_END),'[preproc.loadCSV] Loading CSV Files...')\n",
        "print(train_files_pb)\n",
        "all_labels = []\n",
        "\n",
        "for file in train_files_pb:\n",
        "    file_path = os.path.join(TRAIN_FILES_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "    all_labels.extend(data['type'].unique())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwxeI603PDzr",
        "outputId": "33580d52-3dee-42dc-89b1-ea42fe05649a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[preproc.transform] Transforming Data...: : 201it [00:03, 52.33it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_16984\\2765204885.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...: : 1000it [00:14, 67.72it/s]\n"
          ]
        }
      ],
      "source": [
        "X_all = []\n",
        "y_all = []\n",
        "\n",
        "train_files_pb = tqdm(islice(TRAIN_FILES, TRAIN_FILES_COUNT),'[preproc.transform] Transforming Data...')\n",
        "\n",
        "for file in train_files_pb:\n",
        "    file_path = os.path.join(TRAIN_FILES_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # 转换时间列，提取特征等\n",
        "    data['time'] = pd.to_datetime(data['time'])\n",
        "    data['hour'] = data['time'].dt.hour\n",
        "    data['day_of_week'] = data['time'].dt.dayofweek\n",
        "    data['month'] = data['time'].dt.month\n",
        "\n",
        "    # 使用转换后的标签\n",
        "    data['type_encoded'] = label_encoder.transform(data['type'])\n",
        "\n",
        "    X = data[['lat', 'lon', '速度', '方向', 'hour', 'day_of_week', 'month']]\n",
        "    y = data['type_encoded']\n",
        "\n",
        "    X_all.append(X)\n",
        "    y_all.append(y)\n",
        "\n",
        "# 将所有数据合并为一个大的 DataFrame\n",
        "X = pd.concat(X_all, ignore_index=True)\n",
        "y = pd.concat(y_all, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KmR6JQPHPLyF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XLgbYJ7tPQet"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class FishingVesselDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        \"\"\"\n",
        "        features: 特征数据，尺寸为 (n_samples, n_features)\n",
        "        labels: 标签数据，尺寸为 (n_samples,)\n",
        "        \"\"\"\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# 假设 X_train, y_train, X_test, y_test 已经准备好了\n",
        "# 将数据转换为 PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# 创建 Dataset\n",
        "train_dataset = FishingVesselDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = FishingVesselDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# 创建 DataLoader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ce58116aPUvU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.cuda] Availability: False\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FishingVesselNet(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(FishingVesselNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_features, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        '''\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 实例化模型\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = len(torch.unique(y_train_tensor)) # 假设所有类别都在训练集中出现过\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'[torch.cuda] Availability: {torch.cuda.is_available()}')\n",
        "\n",
        "model = FishingVesselNet(num_features, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dAi6g6rMNavZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [01:00<00:00, 264.92it/s]\n",
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [01:00<00:00, 263.32it/s]\n",
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:59<00:00, 270.10it/s]\n",
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:59<00:00, 269.52it/s]\n",
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:59<00:00, 269.89it/s]\n",
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:59<00:00, 271.32it/s]\n",
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [01:07<00:00, 237.79it/s]\n",
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [01:00<00:00, 266.67it/s]\n",
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [01:00<00:00, 266.87it/s]\n",
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [01:00<00:00, 266.63it/s]\n",
            "[torch.train.epoch] Train Epochs: 100%|██████████| 10/10 [10:07<00:00, 60.72s/it]\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 训练模型迭代次数\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), '[torch.train.epoch] Train Epochs'):\n",
        "    for inputs, labels in tqdm(train_loader, '[torch.train.epoch.single] Single Epoch'):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #print(f' Epoch {epoch+1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:58<00:00, 273.25it/s]\n",
            "[torch.train.epoch] Train Epochs:   5%|▌         | 1/20 [00:58<18:36, 58.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1, Loss: 0.369807630777359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:58<00:00, 272.52it/s]\n",
            "[torch.train.epoch] Train Epochs:  10%|█         | 2/20 [01:57<17:38, 58.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2, Loss: 0.7722816467285156\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:58<00:00, 272.81it/s]\n",
            "[torch.train.epoch] Train Epochs:  15%|█▌        | 3/20 [02:56<16:40, 58.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3, Loss: 0.3232731521129608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:58<00:00, 272.79it/s]\n",
            "[torch.train.epoch] Train Epochs:  20%|██        | 4/20 [03:55<15:41, 58.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4, Loss: 0.4616638720035553\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:59<00:00, 271.47it/s]\n",
            "[torch.train.epoch] Train Epochs:  25%|██▌       | 5/20 [04:54<14:44, 58.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5, Loss: 0.40403589606285095\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:59<00:00, 271.71it/s]\n",
            "[torch.train.epoch] Train Epochs:  30%|███       | 6/20 [05:53<13:45, 58.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6, Loss: 0.4292660057544708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:58<00:00, 272.31it/s]\n",
            "[torch.train.epoch] Train Epochs:  35%|███▌      | 7/20 [06:52<12:46, 58.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7, Loss: 0.48033133149147034\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:59<00:00, 269.53it/s]\n",
            "[torch.train.epoch] Train Epochs:  40%|████      | 8/20 [07:51<11:49, 59.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8, Loss: 0.23125986754894257\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [01:00<00:00, 265.54it/s]\n",
            "[torch.train.epoch] Train Epochs:  45%|████▌     | 9/20 [08:52<10:55, 59.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9, Loss: 0.4116075932979584\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [01:05<00:00, 244.11it/s]\n",
            "[torch.train.epoch] Train Epochs:  50%|█████     | 10/20 [09:58<10:14, 61.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10, Loss: 0.6728768348693848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [01:02<00:00, 258.73it/s]\n",
            "[torch.train.epoch] Train Epochs:  55%|█████▌    | 11/20 [11:00<09:14, 61.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11, Loss: 0.8310718536376953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:58<00:00, 275.70it/s]\n",
            "[torch.train.epoch] Train Epochs:  60%|██████    | 12/20 [11:58<08:04, 60.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12, Loss: 0.5148789882659912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:57<00:00, 278.40it/s]\n",
            "[torch.train.epoch] Train Epochs:  65%|██████▌   | 13/20 [12:56<06:57, 59.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13, Loss: 0.7719016671180725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:57<00:00, 278.93it/s]\n",
            "[torch.train.epoch] Train Epochs:  70%|███████   | 14/20 [13:53<05:54, 59.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14, Loss: 0.2730102837085724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:57<00:00, 279.41it/s]\n",
            "[torch.train.epoch] Train Epochs:  75%|███████▌  | 15/20 [14:51<04:52, 58.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15, Loss: 0.7968931794166565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:58<00:00, 276.67it/s]\n",
            "[torch.train.epoch] Train Epochs:  80%|████████  | 16/20 [15:49<03:53, 58.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 16, Loss: 0.4079136550426483\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:57<00:00, 277.78it/s]\n",
            "[torch.train.epoch] Train Epochs:  85%|████████▌ | 17/20 [16:46<02:54, 58.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 17, Loss: 0.35070183873176575\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:57<00:00, 277.95it/s]\n",
            "[torch.train.epoch] Train Epochs:  90%|█████████ | 18/20 [17:44<01:56, 58.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 18, Loss: 0.5674864649772644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:57<00:00, 277.29it/s]\n",
            "[torch.train.epoch] Train Epochs:  95%|█████████▌| 19/20 [18:42<00:58, 58.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 19, Loss: 0.66657954454422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.epoch.single] Single Epoch: 100%|██████████| 16049/16049 [00:57<00:00, 278.17it/s]\n",
            "[torch.train.epoch] Train Epochs: 100%|██████████| 20/20 [19:40<00:00, 59.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 20, Loss: 0.4079294204711914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), '[torch.train.epoch] Train Epochs'):\n",
        "    for inputs, labels in tqdm(train_loader, '[torch.train.epoch.single] Single Epoch'):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f' Epoch {epoch+1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPbM_G6VbGMj",
        "outputId": "4c79d340-5d35-405b-a329-14be3b741b5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.single] Training Single Epoch 12: 100%|██████████| 16049/16049 [00:58<00:00, 276.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.step] Epoch 12, Loss: 0.624221920967102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train single epoch\n",
        "train_single_pb = tqdm(train_loader)\n",
        "train_single_pb.set_description(f'[torch.train.single] Training Single Epoch {epoch+2}')\n",
        "\n",
        "for inputs, labels in train_single_pb:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "epoch += 1\n",
        "print(f'[torch.train.step] Epoch {epoch+1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "401a0kdTPZx8",
        "outputId": "ec16a73b-1075-4e0a-8745-02bbb6c66c2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.test] Testing Accuracy: 100%|██████████| 10699/10699 [00:13<00:00, 778.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[torch.test] Accuracy on test set: 74.78692144572119%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Test current accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "test_loader_pb = tqdm(test_loader)\n",
        "test_loader_pb.set_description('[torch.test] Testing Accuracy')\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader_pb:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'\\n[torch.test] Accuracy on test set: {100 * correct / total}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "eF6609JifcsB",
        "outputId": "ece78981-349a-4a97-a431-ebc37518e937"
      },
      "outputs": [],
      "source": [
        "# Evaluate Accuracy on each epoch\n",
        "import torch.optim as optim\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 训练模型\n",
        "num_epochs = 15\n",
        "\n",
        "epochs_pb = tqdm(range(num_epochs))\n",
        "epochs_pb.set_description('[torch.train] Training')\n",
        "\n",
        "for epoch in epochs_pb:\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'[torch.train.accuracy] Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {100 * correct / total}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:   5%|▌         | 1/20 [01:07<21:31, 67.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 1, Loss: 0.3575284481048584, Accuracy: 74.9261016117443%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  10%|█         | 2/20 [02:16<20:28, 68.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 2, Loss: 0.7616659998893738, Accuracy: 75.7532962186224%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  15%|█▌        | 3/20 [03:25<19:22, 68.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 3, Loss: 0.6567702293395996, Accuracy: 74.55821615716698%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  20%|██        | 4/20 [04:33<18:15, 68.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 4, Loss: 1.1146769523620605, Accuracy: 75.22899737704535%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  25%|██▌       | 5/20 [05:42<17:06, 68.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 5, Loss: 0.38899847865104675, Accuracy: 75.00029208849114%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  30%|███       | 6/20 [06:49<15:56, 68.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 6, Loss: 0.5774917602539062, Accuracy: 75.16429977626022%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  35%|███▌      | 7/20 [07:58<14:50, 68.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 7, Loss: 0.5234153866767883, Accuracy: 75.31954480929542%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  40%|████      | 8/20 [09:06<13:40, 68.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 8, Loss: 0.5309907793998718, Accuracy: 75.64902062728925%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  45%|████▌     | 9/20 [10:15<12:31, 68.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 9, Loss: 0.5204753279685974, Accuracy: 75.21994263382034%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  50%|█████     | 10/20 [11:23<11:24, 68.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 10, Loss: 0.7448833584785461, Accuracy: 75.40425047172292%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  55%|█████▌    | 11/20 [12:30<10:11, 67.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 11, Loss: 1.0035487413406372, Accuracy: 74.73741244647478%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  60%|██████    | 12/20 [13:37<09:00, 67.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 12, Loss: 0.42967626452445984, Accuracy: 75.34422628679584%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  65%|██████▌   | 13/20 [14:44<07:52, 67.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 13, Loss: 0.5628199577331543, Accuracy: 74.52681664437058%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  70%|███████   | 14/20 [15:51<06:43, 67.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 14, Loss: 0.8336343765258789, Accuracy: 75.10588207803436%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  75%|███████▌  | 15/20 [16:58<05:35, 67.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 15, Loss: 0.622100293636322, Accuracy: 74.90901443501323%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  80%|████████  | 16/20 [18:04<04:27, 66.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 16, Loss: 0.8880559802055359, Accuracy: 74.70469853546831%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  85%|████████▌ | 17/20 [19:10<03:20, 66.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 17, Loss: 0.8210957646369934, Accuracy: 74.35477652309544%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  90%|█████████ | 18/20 [20:17<02:13, 66.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 18, Loss: 0.5295153260231018, Accuracy: 74.8488442058406%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training:  95%|█████████▌| 19/20 [21:23<01:06, 66.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 19, Loss: 0.8045713305473328, Accuracy: 75.68684608689048%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train] Training: 100%|██████████| 20/20 [22:30<00:00, 67.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.train.accuracy] Epoch 20, Loss: 0.37708941102027893, Accuracy: 75.4873496474492%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 20\n",
        "\n",
        "epochs_pb = tqdm(range(num_epochs))\n",
        "epochs_pb.set_description('[torch.train] Training')\n",
        "\n",
        "for epoch in epochs_pb:\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'[torch.train.accuracy] Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {100 * correct / total}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "D5hXwW5vkpXv"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = '../../PR/model.ptm'\n",
        "torch.save(model.state_dict(), MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ksa9cjPmmqTu"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9uoIN21nMUh"
      },
      "outputs": [],
      "source": [
        "model = FishingVesselNet(num_features, num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint = torch.load(MODEL_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "model.eval()\n",
        "# - or -\n",
        "model.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OONmhPHxTCew"
      },
      "source": [
        "# Pattern Recognition 24H1\n",
        "#### Runze Ji, Jiashuo Tian, Ziqian Liu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import necessary Modules\n",
        "* pandas\n",
        "* scikit-learn\n",
        "* itertools.islice\n",
        "* tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "08wd7jgWOOmk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import islice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Specify the parameters used in training the classifier:\n",
        "* Number of training files (TRAIN_FILES_COUNT)\n",
        "* Epochs (EPOCHS)\n",
        "* Path to train files (TRAIN_FILES_PATH)\n",
        "* Path to Model (MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[init] Found 18329 Training Files\n",
            "\n"
          ]
        }
      ],
      "source": [
        "TRAIN_FILES_COUNT = 18000\n",
        "TRAIN_FILES_OFFSET = 0\n",
        "EPOCHS = 20\n",
        "\n",
        "TRAIN_FILES_PATH = '../../PR/train'\n",
        "TEST_DATASET_PATH = '../../PR/test_dataset'\n",
        "MODEL_PATH = '../../PR/model3.ptm'\n",
        "LOGS_PATH = '../../PR/eval3.csv'\n",
        "\n",
        "TRAIN_FILES = os.listdir(TRAIN_FILES_PATH)\n",
        "TRAIN_FILES_END = TRAIN_FILES_COUNT + TRAIN_FILES_OFFSET\n",
        "NUMBER_OF_EPOCHS = 0\n",
        "print(f'[init] Found {len(TRAIN_FILES)} Training Files\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Slice training files and encode labels\n",
        "* All files containing datasets will be sliced in specified count, allowing separate training\n",
        "* Encode 'type'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "19GMj6YsOylz",
        "outputId": "d8d0c30f-0bfa-4665-8acc-9fabfae412b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[init] Reading from Index-0 to Index-17999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[preproc.loadCSV] Loading CSV Files...: 18000it [01:53, 158.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: 刺网 --> Encoded Label: 0\n",
            "Label: 围网 --> Encoded Label: 1\n",
            "Label: 拖网 --> Encoded Label: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(f'[init] Reading from Index-{TRAIN_FILES_OFFSET} to Index-{TRAIN_FILES_END-1}')\n",
        "train_files_pb = tqdm(islice(TRAIN_FILES, TRAIN_FILES_OFFSET, TRAIN_FILES_END), '[preproc.loadCSV] Loading CSV Files...')\n",
        "\n",
        "all_labels = []\n",
        "\n",
        "for file in train_files_pb:\n",
        "    file_path = os.path.join(TRAIN_FILES_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "    all_labels.extend(data['type'].unique())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_labels)\n",
        "\n",
        "# 打印标签和对应的编码\n",
        "for label, encoded_label in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
        "    print(f\"Label: {label} --> Encoded Label: {encoded_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Transforms dataframe and extend datatypes\n",
        "* Analyzes dataframe and extract features\n",
        "* Extend features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwxeI603PDzr",
        "outputId": "33580d52-3dee-42dc-89b1-ea42fe05649a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[preproc.transform] Transforming Data...:   1%|          | 199/18000 [00:02<03:59, 74.32it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  11%|█         | 1955/18000 [00:25<04:35, 58.21it/s] C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  15%|█▌        | 2745/18000 [00:41<03:18, 76.78it/s] C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  18%|█▊        | 3244/18000 [00:48<03:39, 67.37it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  26%|██▌       | 4689/18000 [01:10<02:19, 95.26it/s] C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  38%|███▊      | 6928/18000 [01:47<02:14, 82.45it/s] C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  40%|███▉      | 7154/18000 [01:49<02:13, 80.96it/s] C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  44%|████▍     | 7906/18000 [01:59<01:42, 98.06it/s] C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  56%|█████▋    | 10169/18000 [02:29<01:01, 128.36it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  76%|███████▌  | 13661/18000 [03:07<00:37, 115.21it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  93%|█████████▎| 16693/18000 [03:41<00:11, 112.85it/s]C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  94%|█████████▍| 16897/18000 [03:43<00:12, 89.83it/s] C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...:  96%|█████████▋| 17362/18000 [03:49<00:07, 81.51it/s] C:\\Users\\jirun\\AppData\\Local\\Temp\\ipykernel_26540\\89399510.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data['time'] = pd.to_datetime(data['time'])\n",
            "[preproc.transform] Transforming Data...: 100%|██████████| 18000/18000 [03:58<00:00, 75.49it/s] \n"
          ]
        }
      ],
      "source": [
        "X_all = []\n",
        "y_all = []\n",
        "\n",
        "train_files_pb = tqdm(islice(TRAIN_FILES, TRAIN_FILES_COUNT),'[preproc.transform] Transforming Data...', TRAIN_FILES_COUNT)\n",
        "\n",
        "for file in train_files_pb:\n",
        "    file_path = os.path.join(TRAIN_FILES_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # 转换时间列，提取特征等\n",
        "    data['time'] = pd.to_datetime(data['time'])\n",
        "    data['hour'] = data['time'].dt.hour\n",
        "    #data['day_of_week'] = data['time'].dt.dayofweek\n",
        "    data['month'] = data['time'].dt.month\n",
        "\n",
        "    # 使用转换后的标签\n",
        "    data['type_encoded'] = label_encoder.transform(data['type'])\n",
        "\n",
        "    X = data[['lat', 'lon', '速度', '方向', 'hour', 'month']]\n",
        "    y = data['type_encoded']\n",
        "\n",
        "    X_all.append(X)\n",
        "    y_all.append(y)\n",
        "\n",
        "# 将所有数据合并为一个大的 DataFrame\n",
        "X = pd.concat(X_all, ignore_index=True)\n",
        "y = pd.concat(y_all, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KmR6JQPHPLyF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define dataset structure\n",
        "* Create Datasets for training and verifying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XLgbYJ7tPQet"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class FishingVesselDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        \"\"\"\n",
        "        features: 特征数据，尺寸为 (n_samples, n_features)\n",
        "        labels: 标签数据，尺寸为 (n_samples,)\n",
        "        \"\"\"\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# 假设 X_train, y_train, X_test, y_test 已经准备好了\n",
        "# 将数据转换为 PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# 创建 Dataset\n",
        "train_dataset = FishingVesselDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = FishingVesselDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# 创建 DataLoader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=56, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define Neural Network Structure, Loss Function, and  Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ce58116aPUvU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_features = 6\n",
            "num_classes = 3\n",
            "[torch.cuda] Availability: False\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FishingVesselNet(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(FishingVesselNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_features, 50)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.fc3 = nn.Linear(50, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 实例化模型\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = len(torch.unique(y_train_tensor)) # 假设所有类别都在训练集中出现过\n",
        "\n",
        "print(f'num_features = {num_features}')\n",
        "print(f'num_classes = {num_classes}')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'[torch.cuda] Availability: {torch.cuda.is_available()}')\n",
        "\n",
        "model = FishingVesselNet(num_features, num_classes).to(device)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0015)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = FishingVesselNet(num_features, num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint = torch.load(MODEL_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "NUMBER_OF_EPOCHS = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "##model.eval()\n",
        "# - or -\n",
        "model.train()\n",
        "NUMBER_OF_EPOCHS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train a single epoch and evaluate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPbM_G6VbGMj",
        "outputId": "4c79d340-5d35-405b-a329-14be3b741b5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.single] Training Single Epoch 3:   9%|▉         | 35647/402167 [02:01<21:07, 289.22it/s]"
          ]
        }
      ],
      "source": [
        "# Train single epoch\n",
        "train_single_pb = tqdm(train_loader)\n",
        "train_single_pb.set_description(f'[torch.train.single] Training Single Epoch {NUMBER_OF_EPOCHS + 1}')\n",
        "\n",
        "for inputs, labels in train_single_pb:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "NUMBER_OF_EPOCHS += 1\n",
        "print(f'[torch.train.step] Epoch {NUMBER_OF_EPOCHS}, Loss: {loss.item()}, Total Number of Epochs: {NUMBER_OF_EPOCHS}')\n",
        "\n",
        "# Test current accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "test_loader_pb = tqdm(test_loader)\n",
        "test_loader_pb.set_description('[torch.test] Testing Accuracy')\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader_pb:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'\\n[torch.test] Accuracy on test set: {100 * correct / total}%')\n",
        "\n",
        "torch.save({\n",
        "            'epoch': NUMBER_OF_EPOCHS,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, MODEL_PATH)\n",
        "\n",
        "with open('../../PR/eval.csv', 'a') as eval_file:\n",
        "    eval_file.writelines(f'{NUMBER_OF_EPOCHS},{format(100 * correct / total, \".2f\")},{format(loss.item(), \".2f\")}\\n')\n",
        "    eval_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate Current Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "401a0kdTPZx8",
        "outputId": "ec16a73b-1075-4e0a-8745-02bbb6c66c2e"
      },
      "outputs": [],
      "source": [
        "# Test current accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "test_loader_pb = tqdm(test_loader)\n",
        "test_loader_pb.set_description('[torch.test] Testing Accuracy')\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader_pb:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'\\n[torch.test] Accuracy on test set: {100 * correct / total}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Specified Number of Epochs and Evaluate Accuracy on each Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "eF6609JifcsB",
        "outputId": "ece78981-349a-4a97-a431-ebc37518e937"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[torch.train.single] Training Single Epoch 3:   2%|▏         | 10313/469194 [00:29<21:36, 354.05it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     13\u001b[0m NUMBER_OF_EPOCHS \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\jirun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jirun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train Specified Number of Epochs and Evaluate Accuracy on each Epoch\n",
        "for ep in range(EPOCHS):\n",
        "    train_single_pb = tqdm(train_loader)\n",
        "    train_single_pb.set_description(f'[torch.train.single] Training Single Epoch {NUMBER_OF_EPOCHS + 1}')\n",
        "\n",
        "    for inputs, labels in train_single_pb:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    NUMBER_OF_EPOCHS += 1\n",
        "    print(f'[torch.train.step] Epoch {NUMBER_OF_EPOCHS}, Loss: {loss.item()}, Total Number of Epochs: {NUMBER_OF_EPOCHS}')\n",
        "\n",
        "    # Test current accuracy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    test_loader_pb = tqdm(test_loader, '[torch.test] Testing Accuracy')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader_pb:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'\\n[torch.test.result] Epoch:{NUMBER_OF_EPOCHS}, Loss:{loss.item()}, Accuracy:{100 * correct / total}%\\n')\n",
        "\n",
        "    torch.save({\n",
        "                'epoch': NUMBER_OF_EPOCHS,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                }, MODEL_PATH)\n",
        "\n",
        "    with open(LOGS_PATH, 'a') as eval_file:\n",
        "        eval_file.writelines(f'{NUMBER_OF_EPOCHS},{format(100 * correct / total, \".2f\")},{format(loss.item(), \".2f\")}\\n')\n",
        "        eval_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksa9cjPmmqTu"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "            'epoch': NUMBER_OF_EPOCHS,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_DATASET_PATH = '../../PR/test_dataset'\n",
        "MODEL_PATH = '../../PR/model2.ptm'\n",
        "TEST_FILES = os.listdir(TEST_DATASET_PATH)\n",
        "\n",
        "print(f'[verify] Found {len(TEST_FILES)} Test Files\\n')\n",
        "\n",
        "X_verify_all = []\n",
        "\n",
        "test_files_pb = tqdm(TEST_FILES,'[preproc.transform] Transforming Data...')\n",
        "\n",
        "for file in test_files_pb:\n",
        "    file_path = os.path.join(TEST_DATASET_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # 转换时间列，提取特征等\n",
        "    data['time'] = pd.to_datetime(data['time'])\n",
        "    data['hour'] = data['time'].dt.hour\n",
        "    #data['day_of_week'] = data['time'].dt.dayofweek\n",
        "    data['month'] = data['time'].dt.month\n",
        "\n",
        "    X_verify = data[['渔船ID', 'lat', 'lon', '速度', '方向', 'hour', 'month']]\n",
        "    #X_test = data[['lat', 'lon', '速度', '方向', 'hour', 'day_of_week', 'month']]\n",
        "\n",
        "    X_verify_all.append(X_verify)\n",
        "\n",
        "# 将所有数据合并为一个大的 DataFrame\n",
        "X_verify = pd.concat(X_verify_all, ignore_index=True)\n",
        "X_verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_verify_tensor = torch.tensor(X_verify[['lat', 'lon', '速度', '方向', 'hour', 'day_of_week', 'month']].values, dtype=torch.float32)\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx]\n",
        "    \n",
        "verify_dataset = TestDataset(X_verify_tensor)\n",
        "verify_loader = DataLoader(dataset=verify_dataset, batch_size=64, shuffle=False)\n",
        "    \n",
        "# 实例化模型\n",
        "num_features = X_verify.drop('渔船ID', axis=1).shape[1]\n",
        "num_classes = 3\n",
        "\n",
        "import torch.optim as optim\n",
        "model = FishingVesselNet(num_features, num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "checkpoint = torch.load(MODEL_PATH)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = []\n",
        "verify_loader_pb = tqdm(verify_loader, '[torch.test] Testing Accuracy')\n",
        "with torch.no_grad():\n",
        "    for inputs in verify_loader_pb:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = X_verify['渔船ID'].values\n",
        "\n",
        "# 创建一个DataFrame来存储预测结果\n",
        "predictions_df = pd.DataFrame({\n",
        "    '渔船ID': ids,\n",
        "    'type': predictions\n",
        "})\n",
        "\n",
        "# 显示DataFrame的前几行以确认\n",
        "predictions_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_replaced = predictions_df[['渔船ID', 'type']].replace({0:\"刺网\", 1:\"围网\", 2:\"拖网\"})\n",
        "X_replaced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_final = X_replaced.drop_duplicates(subset='渔船ID', keep='first')\n",
        "X_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_final.to_csv('../../PR/submissions/nn/submission_nn.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

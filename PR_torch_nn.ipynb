{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OONmhPHxTCew"
      },
      "source": [
        "# Pattern Recognition 24H1\n",
        "#### Runze Ji, Jiashuo Tian, Ziqian Liu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import necessary Modules\n",
        "* pandas\n",
        "* scikit-learn\n",
        "* itertools.islice\n",
        "* tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "08wd7jgWOOmk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import islice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Specify the parameters used in training the classifier:\n",
        "* Number of training files (TRAIN_FILES_COUNT)\n",
        "* Epochs (EPOCHS)\n",
        "* Path to train files (TRAIN_FILES_PATH)\n",
        "* Path to Model (MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_FILES_COUNT = 1\n",
        "TRAIN_FILES_OFFSET = 0\n",
        "EPOCHS = 20\n",
        "\n",
        "TRAIN_FILES_PATH = '../../PR/train'\n",
        "TEST_DATASET_PATH = '../../PR/test_dataset'\n",
        "MODEL_PATH = '../../PR/model3.ptm'\n",
        "LOGS_PATH = '../../PR/eval3.csv'\n",
        "\n",
        "TRAIN_FILES = os.listdir(TRAIN_FILES_PATH)\n",
        "TRAIN_FILES_END = TRAIN_FILES_COUNT + TRAIN_FILES_OFFSET\n",
        "NUMBER_OF_EPOCHS = 0\n",
        "print(f'[init] Found {len(TRAIN_FILES)} Training Files\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Slice training files and encode labels\n",
        "* All files containing datasets will be sliced in specified count, allowing separate training\n",
        "* Encode 'type'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "19GMj6YsOylz",
        "outputId": "d8d0c30f-0bfa-4665-8acc-9fabfae412b4"
      },
      "outputs": [],
      "source": [
        "print(f'[init] Reading from Index-{TRAIN_FILES_OFFSET} to Index-{TRAIN_FILES_END-1}')\n",
        "train_files_pb = tqdm(islice(TRAIN_FILES, TRAIN_FILES_OFFSET, TRAIN_FILES_END), '[preproc.loadCSV] Loading CSV Files...')\n",
        "\n",
        "all_labels = []\n",
        "\n",
        "for file in train_files_pb:\n",
        "    file_path = os.path.join(TRAIN_FILES_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "    all_labels.extend(data['type'].unique())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_labels)\n",
        "\n",
        "# 打印标签和对应的编码\n",
        "for label, encoded_label in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
        "    print(f\"Label: {label} --> Encoded Label: {encoded_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Transforms dataframe and extend datatypes\n",
        "* Analyzes dataframe and extract features\n",
        "* Extend features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwxeI603PDzr",
        "outputId": "33580d52-3dee-42dc-89b1-ea42fe05649a"
      },
      "outputs": [],
      "source": [
        "X_all = []\n",
        "y_all = []\n",
        "\n",
        "train_files_pb = tqdm(islice(TRAIN_FILES, TRAIN_FILES_COUNT),'[preproc.transform] Transforming Data...', TRAIN_FILES_COUNT)\n",
        "\n",
        "for file in train_files_pb:\n",
        "    file_path = os.path.join(TRAIN_FILES_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # 转换时间列，提取特征等\n",
        "    data['time'] = pd.to_datetime(data['time'])\n",
        "    data['hour'] = data['time'].dt.hour\n",
        "    #data['day_of_week'] = data['time'].dt.dayofweek\n",
        "    data['month'] = data['time'].dt.month\n",
        "\n",
        "    # 使用转换后的标签\n",
        "    data['type_encoded'] = label_encoder.transform(data['type'])\n",
        "\n",
        "    X = data[['lat', 'lon', '速度', '方向', 'hour', 'month']]\n",
        "    y = data['type_encoded']\n",
        "\n",
        "    X_all.append(X)\n",
        "    y_all.append(y)\n",
        "\n",
        "# 将所有数据合并为一个大的 DataFrame\n",
        "X = pd.concat(X_all, ignore_index=True)\n",
        "y = pd.concat(y_all, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmR6JQPHPLyF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define dataset structure\n",
        "* Create Datasets for training and verifying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLgbYJ7tPQet"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class FishingVesselDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        \"\"\"\n",
        "        features: 特征数据，尺寸为 (n_samples, n_features)\n",
        "        labels: 标签数据，尺寸为 (n_samples,)\n",
        "        \"\"\"\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# 假设 X_train, y_train, X_test, y_test 已经准备好了\n",
        "# 将数据转换为 PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# 创建 Dataset\n",
        "train_dataset = FishingVesselDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = FishingVesselDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# 创建 DataLoader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=56, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define Neural Network Structure, Loss Function, and  Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce58116aPUvU"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FishingVesselNet(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(FishingVesselNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_features, 50)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.fc3 = nn.Linear(50, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 实例化模型\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = len(torch.unique(y_train_tensor)) # 假设所有类别都在训练集中出现过\n",
        "\n",
        "print(f'num_features = {num_features}')\n",
        "print(f'num_classes = {num_classes}')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'[torch.cuda] Availability: {torch.cuda.is_available()}')\n",
        "\n",
        "model = FishingVesselNet(num_features, num_classes).to(device)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0015)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = FishingVesselNet(num_features, num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint = torch.load(MODEL_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "NUMBER_OF_EPOCHS = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "##model.eval()\n",
        "# - or -\n",
        "model.train()\n",
        "NUMBER_OF_EPOCHS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train a single epoch and evaluate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPbM_G6VbGMj",
        "outputId": "4c79d340-5d35-405b-a329-14be3b741b5e"
      },
      "outputs": [],
      "source": [
        "# Train single epoch\n",
        "train_single_pb = tqdm(train_loader)\n",
        "train_single_pb.set_description(f'[torch.train.single] Training Single Epoch {NUMBER_OF_EPOCHS + 1}')\n",
        "\n",
        "for inputs, labels in train_single_pb:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "NUMBER_OF_EPOCHS += 1\n",
        "print(f'[torch.train.step] Epoch {NUMBER_OF_EPOCHS}, Loss: {loss.item()}, Total Number of Epochs: {NUMBER_OF_EPOCHS}')\n",
        "\n",
        "# Test current accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "test_loader_pb = tqdm(test_loader)\n",
        "test_loader_pb.set_description('[torch.test] Testing Accuracy')\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader_pb:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'\\n[torch.test] Accuracy on test set: {100 * correct / total}%')\n",
        "\n",
        "torch.save({\n",
        "            'epoch': NUMBER_OF_EPOCHS,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, MODEL_PATH)\n",
        "\n",
        "with open('../../PR/eval.csv', 'a') as eval_file:\n",
        "    eval_file.writelines(f'{NUMBER_OF_EPOCHS},{format(100 * correct / total, \".2f\")},{format(loss.item(), \".2f\")}\\n')\n",
        "    eval_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate Current Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "401a0kdTPZx8",
        "outputId": "ec16a73b-1075-4e0a-8745-02bbb6c66c2e"
      },
      "outputs": [],
      "source": [
        "# Test current accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "test_loader_pb = tqdm(test_loader)\n",
        "test_loader_pb.set_description('[torch.test] Testing Accuracy')\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader_pb:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'\\n[torch.test] Accuracy on test set: {100 * correct / total}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Specified Number of Epochs and Evaluate Accuracy on each Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "eF6609JifcsB",
        "outputId": "ece78981-349a-4a97-a431-ebc37518e937"
      },
      "outputs": [],
      "source": [
        "# Train Specified Number of Epochs and Evaluate Accuracy on each Epoch\n",
        "for ep in range(EPOCHS):\n",
        "    train_single_pb = tqdm(train_loader)\n",
        "    train_single_pb.set_description(f'[torch.train.single] Training Single Epoch {NUMBER_OF_EPOCHS + 1}')\n",
        "\n",
        "    for inputs, labels in train_single_pb:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    NUMBER_OF_EPOCHS += 1\n",
        "    print(f'[torch.train.step] Epoch {NUMBER_OF_EPOCHS}, Loss: {loss.item()}, Total Number of Epochs: {NUMBER_OF_EPOCHS}')\n",
        "\n",
        "    # Test current accuracy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    test_loader_pb = tqdm(test_loader, '[torch.test] Testing Accuracy')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader_pb:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'\\n[torch.test.result] Epoch:{NUMBER_OF_EPOCHS}, Loss:{loss.item()}, Accuracy:{100 * correct / total}%\\n')\n",
        "\n",
        "    torch.save({\n",
        "                'epoch': NUMBER_OF_EPOCHS,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                }, MODEL_PATH)\n",
        "\n",
        "    with open(LOGS_PATH, 'a') as eval_file:\n",
        "        eval_file.writelines(f'{NUMBER_OF_EPOCHS},{format(100 * correct / total, \".2f\")},{format(loss.item(), \".2f\")}\\n')\n",
        "        eval_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksa9cjPmmqTu"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "            'epoch': NUMBER_OF_EPOCHS,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_DATASET_PATH = '../../PR/test_dataset'\n",
        "MODEL_PATH = '../../PR/model2.ptm'\n",
        "TEST_FILES = os.listdir(TEST_DATASET_PATH)\n",
        "\n",
        "print(f'[verify] Found {len(TEST_FILES)} Test Files\\n')\n",
        "\n",
        "X_verify_all = []\n",
        "\n",
        "test_files_pb = tqdm(TEST_FILES,'[preproc.transform] Transforming Data...')\n",
        "\n",
        "for file in test_files_pb:\n",
        "    file_path = os.path.join(TEST_DATASET_PATH, file)\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # 转换时间列，提取特征等\n",
        "    data['time'] = pd.to_datetime(data['time'])\n",
        "    data['hour'] = data['time'].dt.hour\n",
        "    #data['day_of_week'] = data['time'].dt.dayofweek\n",
        "    data['month'] = data['time'].dt.month\n",
        "\n",
        "    X_verify = data[['渔船ID', 'lat', 'lon', '速度', '方向', 'hour', 'month']]\n",
        "    #X_test = data[['lat', 'lon', '速度', '方向', 'hour', 'day_of_week', 'month']]\n",
        "\n",
        "    X_verify_all.append(X_verify)\n",
        "\n",
        "# 将所有数据合并为一个大的 DataFrame\n",
        "X_verify = pd.concat(X_verify_all, ignore_index=True)\n",
        "X_verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_verify_tensor = torch.tensor(X_verify[['lat', 'lon', '速度', '方向', 'hour', 'day_of_week', 'month']].values, dtype=torch.float32)\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx]\n",
        "    \n",
        "verify_dataset = TestDataset(X_verify_tensor)\n",
        "verify_loader = DataLoader(dataset=verify_dataset, batch_size=64, shuffle=False)\n",
        "    \n",
        "# 实例化模型\n",
        "num_features = X_verify.drop('渔船ID', axis=1).shape[1]\n",
        "num_classes = 3\n",
        "\n",
        "import torch.optim as optim\n",
        "model = FishingVesselNet(num_features, num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "checkpoint = torch.load(MODEL_PATH)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = []\n",
        "verify_loader_pb = tqdm(verify_loader, '[torch.test] Testing Accuracy')\n",
        "with torch.no_grad():\n",
        "    for inputs in verify_loader_pb:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = X_verify['渔船ID'].values\n",
        "\n",
        "# 创建一个DataFrame来存储预测结果\n",
        "predictions_df = pd.DataFrame({\n",
        "    '渔船ID': ids,\n",
        "    'type': predictions\n",
        "})\n",
        "\n",
        "# 显示DataFrame的前几行以确认\n",
        "predictions_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_replaced = predictions_df[['渔船ID', 'type']].replace({0:\"刺网\", 1:\"围网\", 2:\"拖网\"})\n",
        "X_replaced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_final = X_replaced.drop_duplicates(subset='渔船ID', keep='first')\n",
        "X_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_final.to_csv('../../PR/submissions/nn/submission_nn.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
